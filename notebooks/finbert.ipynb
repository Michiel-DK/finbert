{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (4.1.1)\n",
      "Requirement already satisfied: tokenizers==0.9.4 in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from transformers) (0.9.4)\n",
      "Requirement already satisfied: requests in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from transformers) (2021.11.10)\n",
      "Requirement already satisfied: filelock in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from transformers) (3.3.2)\n",
      "Requirement already satisfied: numpy in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from transformers) (1.21.4)\n",
      "Requirement already satisfied: packaging in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from transformers) (21.2)\n",
      "Requirement already satisfied: sacremoses in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from transformers) (0.0.46)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: pyparsing<3,>=2.0.2 in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from requests->transformers) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: joblib in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from sacremoses->transformers) (0.13.2)\n",
      "Requirement already satisfied: click in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\n",
      "Requirement already satisfied: six in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\n",
      "Requirement already satisfied: importlib-metadata in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from click->sacremoses->transformers) (4.8.2)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->transformers) (3.6.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/admin/.pyenv/versions/3.7.7/envs/finbert/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: torch in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy in /Users/admin/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages (from torch) (1.21.4)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/Users/admin/.pyenv/versions/3.7.7/envs/finbert/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#install hugging face transformers and torch\n",
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#import pretrained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "#import pretrained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv and sample => CHANGE PATH\n",
    "headlines_df = pd.read_csv('../data/analyst_ratings_processed.csv')\n",
    "headlines_df = headlines_df.sample(10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Deal Says Citrix Is Not Exploring A Possible Sale',\n",
       " 'ONEOK Partners to Participate in Bakken Day',\n",
       " 'Shares of Broadsoft to Resume Trade at 4:35PM EST',\n",
       " 'Asterias Provides 6 Mo. Data Readout From Its AST-OPC1 Phase 1/2a Clinical Trial, Says Results Continue to Demonstrate Favorable Safety Profile, Potential for Durable Cell Engraftment at the Injury Site and Improved Motor Function',\n",
       " 'eFuture Announces Appointment of Ms. Ping Yu As CFO']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#transform to numpy - shuffle and put only headlines in list\n",
    "headlines_array = np.array(headlines_df)\n",
    "np.random.shuffle(headlines_array)\n",
    "headlines_list = list(headlines_array[:,1])\n",
    "\n",
    "headlines_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize inputs\n",
    "inputs = tokenizer(headlines_list, padding = True, truncation = True, return_tensors='pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3778, -1.8577,  2.5023],\n",
      "        [ 0.4874, -2.4585,  2.1729],\n",
      "        [-0.6469, -1.5176,  2.5074],\n",
      "        [ 2.1141, -2.0579, -1.3233],\n",
      "        [-0.7994, -1.0177,  2.0862],\n",
      "        [ 0.2181,  1.3837, -1.6399],\n",
      "        [ 2.0557, -1.4729, -1.7830],\n",
      "        [-0.2203, -1.6770,  2.2643],\n",
      "        [ 0.4043, -2.4181,  2.0182],\n",
      "        [-0.2195, -1.7844,  2.4427]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "#generate output model\n",
    "outputs = model(**inputs)\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0525, 0.0120, 0.9355],\n",
      "        [0.1551, 0.0082, 0.8368],\n",
      "        [0.0402, 0.0168, 0.9429],\n",
      "        [0.9546, 0.0147, 0.0307],\n",
      "        [0.0507, 0.0408, 0.9085],\n",
      "        [0.2292, 0.7351, 0.0357],\n",
      "        [0.9516, 0.0279, 0.0205],\n",
      "        [0.0756, 0.0176, 0.9068],\n",
      "        [0.1644, 0.0098, 0.8258],\n",
      "        [0.0644, 0.0135, 0.9222]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "#translate outputs to predictions via softmax\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions to df\n",
    "positive = predictions[:, 0].tolist()\n",
    "negative = predictions[:, 1].tolist()\n",
    "neutral = predictions[:, 2].tolist()\n",
    "\n",
    "table = {'Headline':headlines_list,\n",
    "         \"Positive\":positive,\n",
    "         \"Negative\":negative, \n",
    "         \"Neutral\":neutral}\n",
    "      \n",
    "df = pd.DataFrame(table, columns = [\"Headline\", \"Positive\", \"Negative\", \"Neutral\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Positive</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Asterias Provides 6 Mo. Data Readout From Its ...</td>\n",
       "      <td>0.954589</td>\n",
       "      <td>0.014721</td>\n",
       "      <td>0.030690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Courier Corp Holder Gamco Reports 6.97%, Up Fr...</td>\n",
       "      <td>0.951596</td>\n",
       "      <td>0.027925</td>\n",
       "      <td>0.020479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>UPDATE: Transocean and Chevron Moving off Lows...</td>\n",
       "      <td>0.229157</td>\n",
       "      <td>0.735099</td>\n",
       "      <td>0.035745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Top Stocks In The Surety &amp; Title Insurance Ind...</td>\n",
       "      <td>0.164415</td>\n",
       "      <td>0.009777</td>\n",
       "      <td>0.825807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ONEOK Partners to Participate in Bakken Day</td>\n",
       "      <td>0.155090</td>\n",
       "      <td>0.008151</td>\n",
       "      <td>0.836759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Cedar Shopping Centers Closes Purchase of New ...</td>\n",
       "      <td>0.075591</td>\n",
       "      <td>0.017613</td>\n",
       "      <td>0.906796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Marsh Subsidiary Marsh &amp; McLennan To Acquire B...</td>\n",
       "      <td>0.064367</td>\n",
       "      <td>0.013459</td>\n",
       "      <td>0.922174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Deal Says Citrix Is Not Exploring A Possib...</td>\n",
       "      <td>0.052512</td>\n",
       "      <td>0.011955</td>\n",
       "      <td>0.935533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eFuture Announces Appointment of Ms. Ping Yu A...</td>\n",
       "      <td>0.050713</td>\n",
       "      <td>0.040770</td>\n",
       "      <td>0.908517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shares of Broadsoft to Resume Trade at 4:35PM EST</td>\n",
       "      <td>0.040232</td>\n",
       "      <td>0.016843</td>\n",
       "      <td>0.942925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Positive  Negative  \\\n",
       "3  Asterias Provides 6 Mo. Data Readout From Its ...  0.954589  0.014721   \n",
       "6  Courier Corp Holder Gamco Reports 6.97%, Up Fr...  0.951596  0.027925   \n",
       "5  UPDATE: Transocean and Chevron Moving off Lows...  0.229157  0.735099   \n",
       "8  Top Stocks In The Surety & Title Insurance Ind...  0.164415  0.009777   \n",
       "1        ONEOK Partners to Participate in Bakken Day  0.155090  0.008151   \n",
       "7  Cedar Shopping Centers Closes Purchase of New ...  0.075591  0.017613   \n",
       "9  Marsh Subsidiary Marsh & McLennan To Acquire B...  0.064367  0.013459   \n",
       "0  The Deal Says Citrix Is Not Exploring A Possib...  0.052512  0.011955   \n",
       "4  eFuture Announces Appointment of Ms. Ping Yu A...  0.050713  0.040770   \n",
       "2  Shares of Broadsoft to Resume Trade at 4:35PM EST  0.040232  0.016843   \n",
       "\n",
       "    Neutral  \n",
       "3  0.030690  \n",
       "6  0.020479  \n",
       "5  0.035745  \n",
       "8  0.825807  \n",
       "1  0.836759  \n",
       "7  0.906796  \n",
       "9  0.922174  \n",
       "0  0.935533  \n",
       "4  0.908517  \n",
       "2  0.942925  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorted df\n",
    "df.sort_values(by='Positive', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/admin/code/Michiel-DK/finbert/notebooks/models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save weights model\n",
    "torch.save(model.state_dict(), f\"{path}pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(f\"{path}\", local_files_only=True, num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save weights model\n",
    "torch.save(tokenizer, f\"{path}tokenizer.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "stat: path should be string, bytes, os.PathLike or integer, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4a856effe25a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{path}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_fast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0mtokenizer_class_py\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTOKENIZER_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1803\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1804\u001b[0m         return cls._from_pretrained(\n\u001b[0;32m-> 1805\u001b[0;31m             \u001b[0mresolved_vocab_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_configuration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1806\u001b[0m         )\n\u001b[1;32m   1807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m                 \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_configuration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m                 \u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m                 \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m             )\n\u001b[1;32m   1825\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m_from_pretrained\u001b[0;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1875\u001b[0m         \u001b[0;31m# Instantiate tokenizer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1876\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1877\u001b[0;31m             \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minit_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minit_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1878\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m             raise OSError(\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/envs/finbert/lib/python3.7/site-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m         )\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m             raise ValueError(\n\u001b[1;32m    195\u001b[0m                 \u001b[0;34m\"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/lib/python3.7/genericpath.py\u001b[0m in \u001b[0;36misfile\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"\"\"Test whether a path is a regular file\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: stat: path should be string, bytes, os.PathLike or integer, not NoneType"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(f\"{path}\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c49390496f7eb6c96315a82eb38e3ecf2c1e2a9b335fbaa39aa0450a455560ce"
  },
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('finbert': pyenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
